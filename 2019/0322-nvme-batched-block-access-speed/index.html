<?xml version="1.0" encoding="iso-8859-1"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta content="text/html; charset=iso-8859-1" http-equiv="content-type" /><meta name="viewport" content="initial-scale=1"><title>NVMe "Disk" Bandwidth and Latency for Batched Block Requests - panthema.net</title><link rel="stylesheet" type="text/css" href="/css/base.css" /><link rel="stylesheet" type="text/css" href="/css/fonts.css" /><link rel="alternate" type="application/rss+xml" title="panthema.net Weblog Feed RSS 2.0" href="http://panthema.net/xmlfeed/weblog-rss20.xml" /><link rel="alternate" type="application/atom+xml" title="panthema.net Weblog Feed Atom 1.0" href="http://panthema.net/xmlfeed/weblog-atom10.xml" /><script type="text/javascript" src="/js/base.js"></script><script type="text/javascript">/* <!-- */makeIEHover = function() {var lis = document.getElementById('ptnav').getElementsByTagName('li');for (var i = 0; i < lis.length; i++) {lis[i].onmouseover = function() {this.className += ' sfhover';};lis[i].onmouseout = function() {this.className = this.className.replace(new RegExp(' sfhover\b'), '');};}};if (window.attachEvent) window.attachEvent('onload', makeIEHover);/* --> */</script></head><body style="background-image: url(/img/bgfractal1.jpg)"><div class="pttopbar"><div style="float: right"><ul id="ptnav"><li class="top"><a class="ni" href="/"><i class="icon-book"></i> Weblog</a><ul style="margin-left: -6px; margin-top: 4px" class="nx"><li><a href="/2019/">2019</a></li><li><a href="/2018/">2018</a></li><li><a href="/2017/">2017</a></li><li><a href="/2016/">2016</a></li><li><a href="/2015/">2015</a></li><li><a href="/2014/">2014</a></li><li><a href="/2013/">2013</a></li><li><a href="/2012/">2012</a></li><li><a href="/2011/">2011</a></li><li><a href="/2010/">2010</a></li><li><a href="/2009/">2009</a></li><li><a href="/2008/">2008</a></li><li><a href="/2007/">2007</a></li><li><a href="/2006/">2006</a></li><li class="nt"><a href="/2005/">2005</a></li><li class="ns"><a href="/xmlfeed/weblog-rss20.xml">RSS Feed</a></li><li class="nt"><a href="/xmlfeed/weblog-atom10.xml">XML Atom</a></li><li class="ns"><a href="/search.html">Search</a></li></ul></li><li class="top"><a class="ni" href="/tags/"><i class="icon-tags"></i> Tags</a><div style="width: 14em; margin-left: -9em; margin-top: 4px"><a style="font-size: 10pt" href="/tags/algebra.html">algebra</a> <a style="font-size: 16pt" href="/tags/c++.html">c++</a> <a style="font-size: 12pt" href="/tags/code-example.html">code-example</a> <a style="font-size: 14pt" href="/tags/code-snippet.html">code-snippet</a> <a style="font-size: 11pt" href="/tags/coding_tricks.html">coding tricks</a> <a style="font-size: 10pt" href="/tags/compsci.html">compsci</a> <a style="font-size: 10pt" href="/tags/compsci_study_thesis.html">compsci study thesis</a> <a style="font-size: 9pt" href="/tags/crypto-speedtest.html">crypto-speedtest</a> <a style="font-size: 10pt" href="/tags/cryptography.html">cryptography</a> <a style="font-size: 9pt" href="/tags/cryptote.html">cryptote</a> <a style="font-size: 10pt" href="/tags/dissertation.html">dissertation</a> <a style="font-size: 12pt" href="/tags/flex-bison-cpp-example.html">flex-bison-cpp-example</a> <a style="font-size: 16pt" href="/tags/frontpage.html">frontpage</a> <a style="font-size: 14pt" href="/tags/fun.html">fun</a> <a style="font-size: 10pt" href="/tags/graphviz.html">graphviz</a> <a style="font-size: 9pt" href="http://www.hmtg.de">hartmetall</a> <a style="font-size: 9pt" href="/tags/helppc.html">helppc</a> <a style="font-size: 9pt" href="/tags/hifi_selbstbau.html">hifi selbstbau</a> <a style="font-size: 10pt" href="/tags/latex.html">latex</a> <a style="font-size: 10pt" href="/tags/librivox.html">librivox</a> <a style="font-size: 10pt" href="/tags/linux.html">linux</a> <a style="font-size: 9pt" href="/tags/massive-sorting.html">massive-sorting</a> <a style="font-size: 11pt" href="/tags/maths.html">maths</a> <a style="font-size: 9pt" href="/tags/music.html">music</a> <a style="font-size: 10pt" href="/tags/netfundamentals.html">netfundamentals</a> <a style="font-size: 11pt" href="/tags/ns-3.html">ns-3</a> <a style="font-size: 10pt" href="/tags/parallel-string-sorting.html">parallel-string-sorting</a> <a style="font-size: 14pt" href="/tags/parsing.html">parsing</a> <a style="font-size: 9pt" href="/tags/qtsqlview.html">qtsqlview</a> <a style="font-size: 14pt" href="/tags/research.html">research</a> <a style="font-size: 11pt" href="/tags/sdios06.html">sdios06</a> <a style="font-size: 9pt" href="/tags/sdlfractal.html">sdlfractal</a> <a style="font-size: 14pt" href="/tags/sorting.html">sorting</a> <a style="font-size: 11pt" href="/tags/sound_of_sorting.html">sound of sorting</a> <a style="font-size: 10pt" href="/tags/stringology.html">stringology</a> <a style="font-size: 16pt" href="/tags/stx-btree.html">stx-btree</a> <a style="font-size: 16pt" href="/tags/stxxl.html">stxxl</a> <a style="font-size: 16pt" href="/tags/talk.html">talk</a> <a style="font-size: 15pt" href="/tags/thrill.html">thrill</a> <a style="font-size: 10pt" href="/tags/tutorium.html">tutorium</a> <a style="font-size: 16pt" href="/tags/university.html">university</a> <a style="font-size: 13pt" href="/tags/utilities.html">utilities</a> <a style="font-size: 10pt" href="/tags/vncrec.html">vncrec</a> <a style="font-size: 9pt" href="/tags/webdesign.html">webdesign</a> </div></li><li class="top"> <a class="ni" href="#"><i class="icon-cubes"></i> Projects</a> <ul style="margin-left: -8em; margin-top: 4px;" class="nx"> <li><a href="/2009/cryptote/">CryptoTE</a></li> <li><a href="/2009/digup/">digup</a></li> <li><a href="/2013/disk-filltest/">disk-filltest</a></li> <li><a href="/2012/1119-eSAIS-Inducing-Suffix-and-LCP-Arrays-in-External-Memory/">eSAIS-LCP</a></li> <li><a href="/2007/flex-bison-cpp-example/">Flex Bison C++ Example</a></li> <li><a href="/2013/malloc_count/">malloc_count</a></li> <li><a href="/2016/0114-diploma-thesis/">On Bispanning Graphs</a></li> <li><a href="/2013/pmbw/">Parallel Memory Bandwidth</a></li> <li><a href="/2013/parallel-string-sorting/">Parallel String Sorting</a></li> <li><a href="/2014/sqlplot-tools/">SqlPlotTools</a></li> <li><a href="/2013/sound-of-sorting/">The Sound of Sorting</a></li> <li><a href="/2007/stx-btree/">STX B+ Tree</a></li> <li><a href="/2010/stx-cbtreedb/">STX Constant BTreeDB</a></li> <li><a href="/2010/stx-execpipe/">STX ExecPipe</a></li> <li><a href="/2007/stx-exparser/">STX Expression Parser</a></li> <li><a href="/tags/stxxl.html">STXXL 1.4</a></li> <li><a href="/tags/thrill.html">Thrill - Big Data Framework</a></li> <li class="nt"><a href="/2014/vncrec-rgb/">VNCrec RGB 0.4</a></li> <li class="ns"><a href="/search.html">Search</a></li> </ul></li><li class="top"> <a class="ni" href="/publications.html"><i class="icon-graduation-cap"></i> Publications</a></li><li class="top"> <a class="ni" href="/about/"><i class="icon-info-circled"></i> About</a> <ul style="margin-left: -6em; margin-top: 4px" class="nx"> <li><a href="/about/">Timo Bingmann</a></li> <li><a href="/about/impressum.html">Impressum</a></li> </ul></li><li class="top"> <a class="ni" href="/subscribe.html"><i class="icon-rss-squared"></i> Subscribe</a></li></ul></div><div style="padding: 6px"><a href="/">panthema</a>  / <a href="/2019/">2019</a> / <a href="/2019/0322-nvme-batched-block-access-speed/">0322-nvme-batched-block-access-speed</a></div></div><div class="ptcontent"><div style="float: right; clear: right; margin: 12pt 0pt 12pt 12pt; font-size: smaller; text-align: center" class="aimg"><a href="/2019/0322-nvme-batched-block-access-speed/"><img src="/2019/0322-nvme-batched-block-access-speed/thumb.jpg" width="392" height="249" alt="Photo of a Samsung NVMe SSD" title="Photo of a Samsung NVMe SSD" style="border: 1px solid #888888" /></a></div><h1>NVMe "Disk" Bandwidth and Latency for Batched Block Requests</h1><p class="info">Posted on 2019-03-22 16:00 by <a href="/about/">Timo Bingmann</a><a href="http://plus.google.com/+TimoBingmann?rel=author"></a> at <a href="/2019/0322-nvme-batched-block-access-speed/">Permlink</a> with <a href="/2019/0322-nvme-batched-block-access-speed/#notes">0 Comments</a>. Tags: <a href="/tags/c++.html">c++</a> <a href="/tags/stxxl.html">stxxl</a> <a href="/tags/thrill.html">thrill</a></p><div class="textcontent"><p>Last week I had the pleasure of being invited to the <a href="https://www.dagstuhl.de/19111">Dagstuhl seminar 19111 on Theoretical Models of Storage Systems</a>. I gave a talk on the history of <a href="/tags/stxxl.html">STXXL</a> and <a href="/tags/thrill.html">Thrill</a>, but also wanted to include some current developments. Most interesting I found is <b>the gap closing between RAM and disk bandwidth</b> due to the (relatively) new Non-Volatile Memory Express (NVMe) storage devices.</p><p>Since I am involved in many projects using external memory, I decided to perform a simple set of fundamental experiments to compare rotational disks and newer solid-state devices (SSDs). The results were interesting enough to write this blog article about.</p><p>Among the tools of STXXL/FOXXLL there are two benchmarks which perform two distinct access patterns: <b>Scan</b> (<code>benchmark_disks</code>) and <b>Random</b> (<code>benchmark_disks_random</code>).</p><ul><li>In <b>Scan</b> a batch of <i>k</i> sequential blocks of size <i>B</i> are read or written in order.<br /><img src="scan.gif" alt="Batched Scanning Pattern" title="Batched Scanning Pattern" width="720" height="26"/></li><li>In <b>Random</b> a batch of <i>k</i> randomly selected blocks of size <i>B</i> from a span of size <i>N</i> are read or written.<br /><img src="random.gif" alt="Batched Random Access Pattern" title="Batched Random Access Pattern" width="720" height="26"/></li></ul><p>The Scan experiment is probably the fastest access method as it reads or writes the disk (actually: storage device) sequentially. The Random experiment is good to determine the access latency of the disk as it first has to seek to the block and then transfer the data. Notice that the Random experiment does <b>batched block accesses</b> like one would perform in a query/answering system where the next set of random blocks depends on calculations performed with the preceding blocks (like in a B-Tree). This is a different experiment than done by most &quot;throughput&quot; measurement tools which issue a continuous stream of random block accesses.</p><p>In our computing center I selected three different types of storage devices:</p><ul><li><b>HDD-2012</b>: a rotational Western Digital WD1000DHTZ Velociraptor 10000rpm 1&thinsp;TB 3.5&quot; hard disk attached via SATA.<br />Benchmarks from <a href="https://hdd.userbenchmark.com/WD-VelociRaptor-1TB/Rating/1389">UserBenchmark.com</a> rate its sequential bandwidth as 121&ndash;217&thinsp;MiB/s read and 112&ndash;192&thinsp;MiB/s write.</li><li><b>SSD-2013</b>: a NAND-Flash Samsung SSD 840 EVO 1&thinsp;TB (MZ-7TE1T0BW) 2.5&quot; attached via SATA. It has 2D-NAND TLC, 19nm chips by Samsung.<br />Benchmarks from <a href="https://ssd.userbenchmark.com/SpeedTest/1436/Samsung-SSD-840-EVO">UserBenchmark.com</a> show its sequential bandwidth ranging from 348&ndash;508&thinsp;MiB/s read and 152&ndash;488&thinsp;MiB/s write.</li><li><b>NVMe-2016</b>: a NVMe Samsung SSD 960 Pro NVMe 512&thinsp;GB (MZ-V6P512BW) M.2 attached via PCIe x4. It has 3D-NAND MLC, chips by Samsung, 48 Layer (V-NAND v3).<br />Benchmarks from <a href="https://ssd.userbenchmark.com/SpeedTest/182182/Samsung-SSD-960-PRO-512GB">UserBenchmark.com</a> show its sequential bandwidth ranging from 1124&ndash;2639&thinsp;MiB/s read and 648&ndash;1943&thinsp;MiB/s write.</li></ul><p>There are two <b>main questions</b> behind the benchmarks:</p><ol><li>what is a good block size <i>B</i> to achieve high throughput, and </li><li>how many I/O requests <i>k</i> of blocks <i>B</i> have to be batched to fully utilize the storage devices.</li></ol><p>These two parameters are vital when designing and implementing external memory algorithms. The experiment was designed to see how these parameters change with the underlying storage architecture. The batched I/O requests are submitted by FOXXLL/STXXL to the Linux kernel using the asynchronous I/O <code>aio</code> interface. While preparing these experiments, I optimized FOXXLL&apos;s I/O queues to submit batches to the linux kernel instead of individual requests (see commit <code><a href="https://github.com/stxxl/foxxll/commit/c400389f72f70d50076a463e5a3ad20f03689403">foxxll@c400389</a></code>). This patch improved throughput on the fast NVMe by about 6% for random I/O.</p><p>All experiments were run with transfer block size <i>B</i> ranging from 1&thinsp;KiB to 64&thinsp;MiB. According to their specifications sheets, all tested devices internally have 4&thinsp;KiB sectors. The batch size was varied from 1, where only a single request in issued to the device at once, up to 128 for the Scan experiment and up to 8&thinsp;Mi for Random.</p><p>The full result set is available as a <a title="Download storage-performance.pdf (158 KiB)" href="storage-performance.pdf">gnuplot PDF <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA0AAAARCAYAAAAG/yacAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALCQAACwkBEvpHPgAAAAd0SU1FB9wMCgcxLPKvBkUAAAHtSURBVCjPhZLNSiNBEMd/1TOZfCwaPAUE8bYnA543xLyCN08efQJPPoBvYMAn8OLVo0dhIeBBEBEkhyjLZP0AQ4Jmeqa79pCd3njaPzTV3dSvqquqZTAY6OrqKgIgglfFO4dzjvl8ztHR0Y/Ly8ufIkLQ/f29Bp2eqs8yVVW11urt7a2maao7OzsdVQ2MKSOo9/D2hhwe4p2jUqlQr9cxxnB2dnbV6/UCaAI+ncLNDdrphCsRwXtPkiScn59f7e7udlSVOHisrKAbG+j7O3JygrbbNKtVRo0G1lpGoxHdbvdKRJoB0qcn5OIC2dtD7+4wDw+sNZt8Oz7m9+MjjUaD2WwG8H0BpSnS78PWFrq/D/U6/vmZXITP11eq1SpJklCr1QA0RgT6fWQ0wvf7mFZrkXl9HclzYmuZz2YkSUIcL3LEAHpwgHx8YFot0jRlOBzinKMoCqy1bG9vs6wYVdjcZDwe8+v6GuccxhhUFWMMURRRFAXLc4oBBFBVoigKzqU1xoTf8WVO3ntUFRFBRDDGBGuMwVq7qLOEnHNYa/HeB2gZFBHyPCfP83/fyHv/BVp2Ls9lQ8pMcdmlWq1GFEU450LkoigCsPy8+OXlhel0irWWLMvIsow8z1HVUGu5JpMJZeMqQPvv/r+KoujzD0w6NaOXWgCIAAAAAElFTkSuQmCC" alt="storage-performance.pdf" style="margin-bottom: -2px" width="13" height="17"/></a>, the benchmark programs are part of the <a href="https://stxxl.org">STXXL/FOXXLL library</a>, and the <a title="Download storage-results.zip (278 KiB)" href="storage-results.zip">raw result data <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA0AAAARCAYAAAAG/yacAAAAAXNSR0IArs4c6QAAAmhJREFUKM+Nks9LVFEcxT/3vfGNP7LGcpDGqZmrthBahETQQgkEW4WFkdF+RBgqcOOyCFq0qEWLaNGvXZuihS7Uf8B0UZAwEYM49TRNZ/K9mXz63sx7t4U5jkHQd3G5fM/3fDn3ngM1NTo6yv+U+LuRTqcXBwcHO5qbm5mcnGR2dnZlfn6+IxqNektLS//elEqlVmzbVr29vT8TicQ1KWWPlDK0h4dqh5PJJLlcjndv33QLIZZN07ynaZqnlPoKVPbwA6Sbdx9Tp4k2ywuZm+qwYHpqDqFnhRCFWmnVNz16OYEKKidKvvFNHGrj1qXT9D/IbHyaM8uVHU+Rz7Tw+U5TVd7DVxN4rtvlaY3ZtniC4d5Ohp9ksUwrGmlpoFxYxmmP0ulEzu6ISCYE4JZ+nRsevDAXaztGadtn5MUi1moJ6g3q3DzWVp6KdhRfhLuFEIshAFs1nErEopiFLX6UfK73J1kohlkt6zy7PU6gNwDbAJpSaleepR1B13Wa6sMcFqDvgBEWlH0I/ACUD5Xi7icIsUsqBk0A5D3FxpaGo2C9pJGzAT8Av4zu59cV2r5Pq349+e2AtSJsVQSZYoiP6z4504dAgSp6SXfqvi/q1wB3l7TwIWg//7riOZuiaoL4cwhd73AnxwNhZIFZpZRT9UkmE10gegBdoRgYuPh8ZmZ6RCACJUQBpd4Ddi6X20+DlFKXUjYC4VQqtWzbturr6/sej8ePSykbpJTVWb0mc8qyrHI6nf4yNDSUcBwH13WbHce5aprmU8MwvEgkgmVZB9M9Nja2dz1pGMbl1tbWG7FY7IqU8kxtyn8DMLsNssRtKd0AAAAASUVORK5CYII=" alt="storage-results.zip" style="margin-bottom: -2px" width="13" height="17"/></a> (278 KiB) can be downloaded as a zip file.</p><h2>Scan Bandwidth Results</h2><p>The following three plots show the results for the Scan experiment on the three devices (click on the images for larger versions):</p><p style="text-align: center"><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-02.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-02.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-04.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-04.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-06.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-06.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a></p><p>The block size is plotted on the x-axis and the y-axis presents the achieved bandwidth for the access pattern. The colored series visible as parallel lines are exponentially increasing batch size <i>k</i>. Solid lines are read operations and dashed lines are write operations.</p><p>The results clearly show that achieving maximum bandwidth on the devices requires a certain combination of block size <i>B</i> and batch size <i>k</i>. </p><p>For HDD-2012 it is sufficient use 32&thinsp;KiB blocks with <i>k</i>=1 (solid red line) or batch size <i>k</i>=64 with 4&thinsp;KiB blocks (solid black line) to achieve the maximum bandwidth of about 190&thinsp;MiB/s. Due to caching in the drive itself, the initial bandwidth series actually overshoot the maximum drive bandwidth. Write bandwidth is approximately the same as read bandwidth for this disk; except for blocks smaller than 4&thinsp;KiB, which are smaller than the drive&apos;s internal sectors.</p><p>SSD-2013 is a considerably faster storage device and reaches the ~500&thinsp;MiB/s and ~480&thinsp;MiB/s reported by other user benchmarks on UserBenchmark.com. However, the <i>B</i> and <i>k</i> values to achieve maximum bandwidth have changed considerably: when performing single operations (<i>k</i>=1) the block size <i>B</i> has to be increased to 32&thinsp;MiB to achieve the 500&thinsp;MiB/s throughput. The block size can be reduced by batching operations: the biggest batches in our experiment <i>k</i>=128 achieve maximum throughput for <i>B</i>=64&thinsp;KiB, which is only 8&thinsp;MiB per batch. For smaller batch sizes and smaller block sizes the performance degrades gradually. For example, single operations with 64&thinsp;KiB or 128&thinsp;KiB blocks only reach ~45% of maximum bandwidth. Write operations again behave very similar to read operation, though the maximum bandwidth peaks at ~480&thinsp;MiB/s.</p><p>NVMe-2016 shows another boost in absolute bandwidth and a new disparity in read and write performance. For single operations (<i>k</i>=1) the block size has to be increased to 64&thinsp;MiB to nearly reach the maximum throughput measured in our experiment. The maximum read bandwidth of ~3400&thinsp;MiB/s was higher than the those from UserBenchmark.com (only 2639&thinsp;MiB/s) while the write bandwidth of ~1900&thinsp;MiB/s is as expected. This may be due to Linux&apos;s better support for the NVMe I/O queues than the Windows kernel. For <i>k</i>=128 batch sizes the block size required to read full bandwidth was <i>B</i>=512&thinsp;KiB or <i>B</i>=1&thinsp;MiB (which are 64&thinsp;MiB or 128&thinsp;MiB batches). The maximum write bandwidth is much lower, only about 55% of read bandwidth, which is an interesting disparity. The NVMe plot shows how difficult it is to fully utilize the available sequential read bandwidth of the current NVMe generation.</p><p>The following plot shows the vast increase in bandwidth of NVMe and SSD technology over rotational disks: NVMe devices are more than an order of magnitude faster than rotational disks.</p><p style="text-align: center"><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-07.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-07.png" width="666" height="500" class="zoom" style="max-width:60%; height: auto" /></a></p><h2>Random Disk Access Latency and Bandwidth Results</h2><p>The next series of plots shows the results of the Random experiment. The following two first focus on the latency of each batch request. The block size is again plotted on the x-axis and the y-axis presents the latency per block for the Random access pattern.</p><p style="text-align: center"><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-08.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-08.png" width="800" height="600" class="zoom" style="max-width:39%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-09.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-09.png" width="800" height="600" class="zoom" style="max-width:39%; height: auto" /></a></p><p>The plot on the left shows that a single read operation on HDD-2012 has a latency of 5.6&thinsp;ms until around 64&thinsp;KiB after which the latency starts rising linearly with the block size. Write latency on HDD-2012 is only 1.5&thinsp;ms due to write caching inside the disk.</p><p>On SDD-2013 the read latency is only around 170&thinsp;&micro;s for 4&thinsp;KiB blocks, but it rises quickly to 912&thinsp;&micro;s for 128&thinsp;KiB blocks and then rises linearly with the number of sectors the SSD has to access. Write latency is again lower due to caching and starts with 85&thinsp;&micro;s for 4&thinsp;KiB blocks and 835&thinsp;&micro;s for 128&thinsp;KiB blocks.</p><p>The latency on NVMe-2016 is again lower than on the older SSD: 86&thinsp;&micro;s for 4&thinsp;KiB blocks and 300&thinsp;&micro;s for 128&thinsp;KiB blocks. The linear rise starts earlier than on SSD-2013, which means that the overhead of small block reads has decreased. Write latency on the NVMe device is much lower: only 33&thinsp;&micro;s for 4&thinsp;KiB blocks and 198&thinsp;&micro;s for 128&thinsp;KiB blocks.</p><p>The plot on the right shows the block access latency when performing batched block accesses. Batching allows the storage devices to optimize the access of many random requests by reordering them or executing them in parallel. However, submitting batches of 1024 random requests at once does not yield a 1024-fold speedup. The speedup on the NVMe is only about 25.</p><p>To better understand how well batching speeds up random accesses, consider the following two plots.</p><p style="text-align: center"><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-10.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-10.png" width="800" height="600" class="zoom" style="max-width:39%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-11.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-11.png" width="800" height="600" class="zoom" style="max-width:39%; height: auto" /></a></p><p>In the left plot, the block size is again plotted on the x-axis and the y-axis presents the speedup of submitting 1024 random requests over just a single request in a batch. This plot is the ratio between the previous two latency plots above.</p><p>Depending on the block size, the storage devices can process many blocks in parallel. For 4&thinsp;KiB blocks, the NVMe-2016 can process about 25, the SSD-2013 about 11, and the HDD-2012 about 3 I/O requests simultaneously. This number decreases rapidly for the NVMe-2016 and SSD-2013 as the block size is increased, the HDD-2012 can perform 3 I/O simultaneously up to 32&thinsp;KiB blocks.</p><p>The right plot answers the question of how many I/O requests one has to submit to achieve the maximum batch parallelism for 4&thinsp;KiB blocks. In this plot the x-axis is the batch size <i>k</i> and the y-axis shows the achieved speedup. From that plot one can see that to actually gain the maximum ~27 fold parallelism with the NVMe-2016, one has to submit about 4096 parallel requests. For the SSD-2013 the maximum speedup is ~11 and this requires only 256 parallel requests. And for HDD-2012 the maximum speedup is ~3 and requires 512 requests.</p><p>Both plots show that batching write requests also improves performance, but as not much as for read.</p><p>The following set of six plots shows the random access bandwidth (instead of latency). As before, the block size is plotted on the x-axis and the y-axis presents the bandwidth in MiB/s.</p><p style="text-align: center"><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-12.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-12.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-14.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-14.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-16.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-16.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a><br /><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-13.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-13.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-15.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-15.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-17.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-17.png" width="380" height="285" class="zoom" style="max-width:32%; height: auto" /></a></p><p>One can see a very different shape in the first plot column for HDD-2012 compared to the other two columns: increasing the batch size has a much smaller effect on the throughput than increasing the block size. This is of course due to the rotational disk technology. For random writing on HDD-2012 batching has practically no effect on the throughput. The sequential bandwidth (about 217&thinsp; and 190&thinsp;MiB/s, respectively) is not reached for read and barely for write with very large block sizes.</p><p>For SSD-2013, batching random read operations is very important: this plot has the lowest relative performance for non-batched operations over large batched ones. However, given enough batched operations, for SSD-2013 the throughput of random reads and writes is equal to the throughput for sequential access in the first experiment. Here batching also accelerates writing. If required to use a small block size, the best trade-off on SSD-2013 is to use 64&thinsp;KiB blocks were read is nearing its peak performance and write is already maximized.</p><p>As one would expect, for NVMe-2016, batching random reads is also very important. However, it only reads about 1800&thinsp;MiB/s random throughput for <i>B</i>&lt;32&thinsp;MiB compared to the ~3400&thinsp;MiB/s for sequential access. The same effect appears for random write operations, which peak at about 1300&thinsp;MiB/s compared to ~1900&thinsp;MiB/s for sequential write. Batching is much more important for read operations, than write operations on NVMe-2016, so much so that batched random write of 128&thinsp;KiB blocks is faster than batched random reading of the same block size. Nevertheless with random operations on NVMe-2016 one cannot achieve the same performance as with sequential ones, contrarily to SSD-2013. By increasing the block size to 32 or 64&thinsp;MiB the throughput strangely starts to increase again for large read batches.</p><h2>Conclusion</h2><p>The following two summary plots give an overview of both Read and Random access results, for batch with <i>k</i>=1 on the left and <i>k</i>=1024 on the right.</p><p style="text-align: center"><a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-19.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-19.png" width="800" height="600" class="zoom" style="max-width:39%; height: auto" /></a>&nbsp; <a href="/2019/0322-nvme-batched-block-access-speed/storage-performance-20.png"><img src="/2019/0322-nvme-batched-block-access-speed/storage-performance-20.png" width="800" height="600" class="zoom" style="max-width:39%; height: auto" /></a></p><p>While the two plots highlight the massive increase of NVMe bandwidth over previous storage technology, they also show the shortcomings of NVMe devices: the performance disparity between read and write operations, and also between sequential and random access throughput. Both traits are important when designing algorithms using external memory.</p>In the context of NVMe one must consider that the scanning bandwidth of RAM on machines like this (refer to my <a href="/2013/pmbw/">pmbw project</a> for details), is about 20&thinsp;GiB/s for a single thread and 100&thinsp;GiB/s for all cores in parallel. Thus the ratio between RAM and external storage bandwidth has shrunk by an order of magnitude due to the new technology. This is exciting because external memory algorithms now have to be improved again to cope with the fast storage devices. </div><br/><a id="notes"></a><div class="commentframe" style="display: none; border-color: red" id="newpost_div"> <div class="commentimage"> <a href="http://www.gravatar.com/" id="newpost_imgurl"> <img id="newpost_img" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAAAXNSR0IArs4c6QAAAApJREFUCB1j+A8AAQEBADZfZ4AAAAAASUVORK5CYII=" width="80" height="80" alt="Gravatar" /> </a> </div> <div class="commenttitle">Comment by <b><span id="newpost_name">Name</span></b> at <span id="newpost_date">Date</span><span id="newpost_uri">- URI</span><a href="/2019/0322-nvme-batched-block-access-speed/" rel="nofollow"></a></div> <div class="commentbody" id="newpost_body"></div> <div class="commentbody" style="text-align: right"><big>Preview (Not Saved!)</big></div></div><a id="postnote"></a><form id="noteform" method="post" action="/sendpost.php"><div class="commentform" style="text-align: center"><table class="center thinframe" style="text-align: left"> <tr> <td colspan="2" style="text-align: center">Post Comment<input type="hidden" name="previewok" value="0" /><input type="hidden" name="posturi" value="/2019/0322-nvme-batched-block-access-speed/" /></td> </tr> <tr> <td>Name:</td> <td><input type="text" name="postname" size="40" /></td> </tr> <tr> <td>E-Mail or Homepage:<br /><span style="font-size: 8pt">&nbsp;</span></td> <td><input type="text" name="postmail" size="40" /><br /> <span style="font-size: 8pt">URLs (http://...) are displayed, e-mails are hidden and used for Gravatar.</span></td> </tr> <tr> <td colspan="2" style="text-align: center"> <textarea name="postbody" cols="80" rows="8"></textarea><br /> <span style="font-size: 8pt">Many common HTML elements are allowed in the text, but no CSS style.</span> </td> </tr> <tr> <td colspan="2" style="text-align: center"> <input type="button" name="Preview" value="Preview" onclick="return noteform_submit()" /> <input type="submit" name="Submit" value="Submit" disabled="disabled" id="newpost_submit" /> </td> </tr></table></div></form></div><div style="text-align: right; padding: 8pt; font-size: 12px"><a href="/xmlfeed/weblog-rss20.xml"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPAgMAAAAOp6AcAAAADFBMVEVmZmb/ZgD///+JjnlbTUWmAAAACXBIWXMAAABIAAAASABGyWs+AAAACXZwQWcAAABQAAAADwCexKkxAAAAWUlEQVQYlWNgwAa0VmGAFQyqoSCQ/x8J/CBFMCt0ZdTU/F/7/+1HEkydFpoKVPn+93sUlUujQvP/7UcRTAuNDJ2a//s/imDqtKVZofm/1qGYSZk7sfodGwAAiobBJtffoNAAAAAASUVORK5CYII=" alt="RSS 2.0 Weblog Feed" width="80" height="15"/></a> <a href="/xmlfeed/weblog-atom10.xml"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPAgMAAAAOp6AcAAAADFBMVEVmZmb/ZgD///+JjnlbTUWmAAAAaklEQVQYlWNgwAa0VmGAFQyqoSCQ/x8J/CBFMDV16rTQ/H/7V//69+//v/3/9oMEI6cuAwr+fv/+/b5d///v//0eJBgWOhMo+Gs9UPAdXDBy6lSYyn9wQZCZS0GCYDPXQcykzJ1Y/Y4NAABKvL80wyoMPgAAAABJRU5ErkJggg==" alt="Atom 1.0 Weblog Feed" width="80" height="15"/></a> <a href="http://validator.w3.org/check?uri=referer"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPCAMAAABEF7i9AAAB1FBMVEUHQIgIRY0MN4QNRYoSRYsXUpIYVJQeUpUhWpgjUJMjWpgmU5QnXJkuYZ0uYpwwV5YyZJ4zaKM3ZaA5aaI6aqM6a6E8ZqA8Z6BAbaRBbqdCbaVHd61KSkpKcaZQUFBUf69UgbBVVVVXfq9Yg7NZhbJZh7RagrNeXVxeh7RfjLdgYGBiiLZmZmZnZ2doirhpkbt0m8F3m8B3ncJ5lb55ocR+ocSDgX+Do8eEpMqGhoaHp8iIpMeJjnmJpseKqMeOqsqPsdCQrcyVlZWVsM2Zs9CZtNCfu9ahoaGhuNShudSivNSjtMWjvNSmpqWnpqSrw9qvxNqyxdmyxdyzsK6zs7Ozx9y0xt21yN22tra4y+K80eK/v7/AwMDExMTEy9DH2ejK2+jM2ujN3uvP0dLR0NDR3uvS3uvS3+zU4u3V1dXY2dnY4u7Y4+3b5e/d3d3d5u/d5/De3t7f39/h4uLj4uHj6/Ll7PXm7fTn7/Xo7/Tp7/bq7Ozr6unr8ffw8PDw9vvx9fny+P7z8/Pz9vnz9vv3+fv4+vz5+vz6+vr6+/36/P36///7+/77/Pz7/f37/f78/Pz8/f38/f79/f39/f79/f/9/v79///+/v7+/v////3///+tYR2LAAABKklEQVQ4y2PQoTJg0JlNVYDPQBsyAMTAqaFes8tdsmdPCtA3MDStr3GKKK6myMAprgJ9pcpWs/MlRExYJBvNZLR0sigycHYOV1S7mtjsQJ4gf+HIFPXoWMs6qIGzIRiKZs9GFoEaMRuZhhrYqqJXy8uZYauRxC/VHew7e3YTPAxnI5swG6JxNrKBNjB1EFmIgV0+HA6C8pqqzhXG7BbJJROQIwViMYQ7G2oDQQNn57JyG7kxiCfObrBmS5+BHMuYLkTzMnYDm5WY4lOZZRPKZocxes+aPbuwCFsYohiICLvZsKBAMrDTQ7K/StveU8icTzrOL8JdLpOyWJ49u8Bx9ozw8DY7UYW03hhdxZAOSg2c3jN79rSJs1vSKmfOnpFXNJmynELlvExlAADnaZpLFGEC2wAAAABJRU5ErkJggg==" alt="Valid XHTML 1.1" width="80" height="15"/></a> <a href="http://jigsaw.w3.org/css-validator/check/referer"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPCAMAAABEF7i9AAAB1FBMVEUHQIgIRY0MN4QNRYoSRYsXUpIYVJQeUpUhWpgjUJMjWpgmU5QnXJkuYZ0uYpwwV5YyZJ4zaKM3ZaA5aaI6aqM6a6E8ZqA8Z6BAbaRBbqdCbaVHd61KSkpKcaZQUFBUf69UgbBVVVVXfq9Yg7NZhbJZh7RagrNeXVxeh7RfjLdgYGBiiLZmZmZnZ2doirhpkbt0m8F3m8B3ncJ5lb55ocR+ocSDgX+Do8eEpMqGhoaHp8iIpMeJjnmJpseKqMeOqsqPsdCQrcyVlZWVsM2Zs9CZtNCfu9ahoaGhuNShudSivNSjtMWjvNSmpqWnpqSrw9qvxNqyxdmyxdyzsK6zs7Ozx9y0xt21yN22tra4y+K80eK/v7/AwMDExMTEy9DH2ejK2+jM2ujN3uvP0dLR0NDR3uvS3uvS3+zU4u3V1dXY2dnY4u7Y4+3b5e/d3d3d5u/d5/De3t7f39/h4uLj4uHj6/Ll7PXm7fTn7/Xo7/Tp7/bq7Ozr6unr8ffw8PDw9vvx9fny+P7z8/Pz9vnz9vv3+fv4+vz5+vz6+vr6+/36/P36///7+/77/Pz7/f37/f78/Pz8/f38/f79/f39/f79/f/9/v79///+/v7+/v////3///+tYR2LAAABEUlEQVQ4y2PQoTJg0JlNVYDPQBsyAMTAqaFes8tdsmdPCtA3MDStr3GKKK6myMAprgJ9pcpWs/MlRExYJBvNZLR0sigycHYOV1S7mtjsQJ4gf+HIFPXoWMs6mIEQCsIjwhKoga0qerW8nBm2Gkn8Ut3BvrNnN8HCcDYM2cBIogzs8uFwEJTXVHWuMGa3SC6ZgIgUqCEwdxJr4OxcVm4jNwbxxNkN1mzpM2ajGIjsPGK9PLtZiSk+lVk2oWx2GKP3rNmzC4tQwpDoIIQb2Okh2V+lbe8pZM4nHecX4S6XSVksz55d4Dh7Rnh4m52oQlpvjK5iSAelBk7vmT172sTZLWmVM2fPyCuaTFlOoXJepjIAANlljDGY/l5pAAAAAElFTkSuQmCC" alt="Valid CSS (2.1)" width="80" height="15"/></a><br/>Copyright 2005-2019 <a href="/about/">Timo Bingmann</a> - <a href="/about/impressum.html">Impressum</a></div><script type="text/javascript">var _paq = _paq || []; _paq.push(["trackPageView"]); _paq.push(["enableLinkTracking"]);(function() {var u=(("https:" == document.location.protocol) ? "https" : "http") + "://panthema.net/wik-331/"; _paq.push(["setTrackerUrl", u+"js/"]); _paq.push(["setSiteId", "2"]);var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0]; g.type="text/javascript";g.defer=true; g.async=true; g.src=u+"js/"; s.parentNode.insertBefore(g,s);})();</script><noscript><p><img src="http://panthema.net/wik-331/js/?idsite=2&amp;rec=1" style="border:0" alt="" /></p></noscript></body></html>